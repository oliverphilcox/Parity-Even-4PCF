{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os,sys,pickle\n",
    "from scipy.stats import percentileofscore, chi2, norm\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import loggamma, spherical_jn, sici\n",
    "from scipy.integrate import simps\n",
    "from sympy.physics.wigner import wigner_3j\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even-Parity BOSS 4PCF Analysis\n",
    "This notebook contains the main analysis routines used in Philcox++ (2021) to probe gravitational non-Gaussianity in the isotropic connected 4-point correlation function (4PCF) of the BOSS CMASS sample. For questions, email [Oliver Philcox](mailto:ohep2@cantab.ac.uk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directories (containing pre-computed BOSS and Patchy results)\n",
    "data_dir = '/home/ophilcox/Parity-Even-4PCF/data/'\n",
    "\n",
    "# Output plotting directory\n",
    "plot_dir = '/home/ophilcox/Parity-Even-4PCF/figs/'\n",
    "if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "\n",
    "# Binning parameters\n",
    "R_min = 20\n",
    "R_max = 160\n",
    "n_r = 10\n",
    "\n",
    "# If True, replace the BOSS data with a single Patchy mock (for blinding reasons)\n",
    "fake_data = False\n",
    "\n",
    "# If true, remove any bins separated by <= 14 Mpc/h\n",
    "apply_cuts = False\n",
    "\n",
    "# If true, subtract the mean of the signal\n",
    "remove_mean = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load 4PCFs\n",
    "- These are computed with the [*encore*](https://github.com/oliverphilcox/encore) code of [Philcox et al. 2021](https://arxiv.org/abs/2105.08722)\n",
    "- We use 10 linearly-spaced radial bins in the range [20, 160] $h^{-1}\\mathrm{Mpc}$\n",
    "- Multiplets, i.e. choices of $\\{\\ell_1,\\ell_2,\\ell_3\\}$, are computed up to $\\ell_\\mathrm{max} = 5$, with $\\ell_i=5$ multiplets only being used for edge-correction.\n",
    "- Here we consider only bins with $\\ell_i\\leq 4$ to keep the dimensionality reasonable.\n",
    "- We analyze both the BOSS data, in the CMASS NGC and SGC regions, as well as 1000 MultiDark-Patchy mocks.\n",
    "- All data is provided in the ```data/``` directory. Note that this additionally includes the isotropic 2PCF and 3PCF measurements from BOSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_mocks: 1000\n",
      "N_Lambda: 69\n",
      "N_radial: 120\n"
     ]
    }
   ],
   "source": [
    "binner = lambda bins: (0.5+bins)*(R_max-R_min)/n_r+R_min\n",
    "\n",
    "### First Load Patchy data\n",
    "with np.load(data_dir+'all_patchy_fourpcf.npz') as d:\n",
    "    # SGC measurements\n",
    "    fourpcfSall=d['fourpcfSall']\n",
    "    # NGC measurements\n",
    "    fourpcfNall=d['fourpcfNall']\n",
    "    # {r1, r2, r3}\n",
    "    radii=d['radii']\n",
    "    # {bin-index 1, bin-index 2, bin-index 3}\n",
    "    bins=d['bins']\n",
    "    # {ell-1, ell-2, ell-3}\n",
    "    ells=d['ells']\n",
    "    # SGC disconnected measurements\n",
    "    fourpcfSdisc=d['fourpcfSdisc']\n",
    "    # NGC disconnected measurements\n",
    "    fourpcfNdisc=d['fourpcfNdisc']\n",
    "\n",
    "# Compute the connected-only 4PCF for Patchy\n",
    "fourpcfN = np.asarray(fourpcfNall) - np.asarray(fourpcfNdisc)\n",
    "fourpcfS = np.asarray(fourpcfSall) - np.asarray(fourpcfSdisc)\n",
    "\n",
    "n_radial = len(bins[0])\n",
    "n_mult = len(ells[0])\n",
    "\n",
    "### Now load the BOSS data\n",
    "\n",
    "def load_boss(patch,discon=False,return_all=False):\n",
    "    \"\"\"Load precomputed 4PCFs from BOSS, using either the full or disconnected estimators.\n",
    "    This loads data from the NGC or SGC, here labelled 'N' or 'S'.\"\"\"\n",
    "    if discon:\n",
    "        infile = data_dir+'boss_cmass%s.zeta_discon_4pcf.txt'%patch\n",
    "    else:\n",
    "        infile = data_dir+'boss_cmass%s.zeta_4pcf.txt'%patch\n",
    "    bins1,bins2,bins3 = np.asarray(np.loadtxt(infile,skiprows=3,max_rows=3),dtype=int)\n",
    "    ell1,ell2,ell3 = np.asarray(np.loadtxt(infile,skiprows=9)[:,:3],dtype=int).T\n",
    "    fourpcf_boss = np.loadtxt(infile,skiprows=9)[:,3:]\n",
    "    r1_4pcf = binner(bins1)\n",
    "    r2_4pcf = binner(bins2)\n",
    "    r3_4pcf = binner(bins3)  \n",
    "    if return_all:\n",
    "        return [r1_4pcf,r2_4pcf,r3_4pcf],[bins1,bins2,bins3],[ell1,ell2,ell3],fourpcf_boss\n",
    "    else:\n",
    "        return fourpcf_boss\n",
    "\n",
    "if fake_data:\n",
    "    # Replace the BOSS data with a single Patchy mock for blinding\n",
    "    print(\"Replacing BOSS data with first Patchy mock\")\n",
    "    fourpcf_bossN = fourpcfN[0]\n",
    "    fourpcf_bossS = fourpcfS[0]\n",
    "    fourpcf_bossNdisc = fourpcfNdisc[0]\n",
    "    fourpcf_bossSdisc = fourpcfSdisc[0]\n",
    "    fourpcfN = fourpcfN.copy()[1:]\n",
    "    fourpcfS = fourpcfS.copy()[1:]\n",
    "else:\n",
    "    # Load the BOSS data\n",
    "    radii_boss, bins_boss, ells_boss, fourpcf_bossNall = load_boss('N',return_all=True)\n",
    "    fourpcf_bossSall = load_boss('S')\n",
    "    fourpcf_bossNdisc = load_boss('N',discon=True)\n",
    "    fourpcf_bossSdisc = load_boss('S',discon=True)\n",
    "    \n",
    "    # Remove any odd multipoles!\n",
    "    odds = np.asarray([(-1)**(ells_boss[0][i]+ells_boss[1][i]+ells_boss[2][i])==-1 for i in range(len(fourpcf_bossNall))])\n",
    "    fourpcf_bossNall = np.asarray(fourpcf_bossNall)[~odds]\n",
    "    fourpcf_bossSall = np.asarray(fourpcf_bossSall)[~odds]\n",
    "\n",
    "    fourpcf_bossN = np.asarray(fourpcf_bossNall) - np.asarray(fourpcf_bossNdisc)\n",
    "    fourpcf_bossS = np.asarray(fourpcf_bossSall) - np.asarray(fourpcf_bossSdisc)\n",
    "\n",
    "if remove_mean:\n",
    "    # Remove the mean signal for testing\n",
    "    print(\"Removing mean of data\")\n",
    "    meanN = np.mean(fourpcfN,axis=0)\n",
    "    meanS = np.mean(fourpcfS,axis=0)\n",
    "    for i in range(len(fourpcfN)):\n",
    "        fourpcfN[i] = fourpcfN[i]+meanN*-1\n",
    "        fourpcfS[i] = fourpcfS[i]+meanS*-1\n",
    "    fourpcf_bossN = fourpcf_bossN+meanN*-1\n",
    "    fourpcf_bossS = fourpcf_bossS+meanS*-1\n",
    "    \n",
    "n_mocks = len(fourpcfN)\n",
    "assert n_mocks == len(fourpcfS)\n",
    "\n",
    "print(\"N_mocks: %d\"%n_mocks)\n",
    "print(\"N_Lambda: %d\"%n_mult)\n",
    "print(\"N_radial: %d\"%n_radial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Apply Data Cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the cuts described above for the angular bins.\n",
    "- We can optionally also filter out any bins with internal separations less than 14 Mpc/h, which are diffiult to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Lambda: 42\n",
      "N_bins: 120\n",
      "N_total: 5040\n"
     ]
    }
   ],
   "source": [
    "# filter out ells which are not properly edge-corrected\n",
    "LMAX=4\n",
    "ang_filt = np.asarray(np.logical_and(np.logical_and(ells[0]<=LMAX,ells[1]<=LMAX),ells[2]<=LMAX))\n",
    "\n",
    "# Optionally remove any bins within 14 Mpc of each other\n",
    "dr = (R_max-R_min)/n_r\n",
    "radial_filt = ((radii[1]-radii[0])>1.9*n_r)&((radii[2]-radii[1])>1.9*n_r)\n",
    "if apply_cuts:\n",
    "    print(\"Applying radial binning cuts\")\n",
    "else:\n",
    "    radial_filt = np.ones_like(radial_filt)\n",
    "\n",
    "# Apply filters\n",
    "filt_fourpcfN = np.asarray([ff[ang_filt][:,radial_filt] for ff in fourpcfN])\n",
    "filt_fourpcfS = np.asarray([ff[ang_filt][:,radial_filt] for ff in fourpcfS])\n",
    "filt_fourpcf_bossN = fourpcf_bossN[ang_filt][:,radial_filt]\n",
    "filt_fourpcf_bossS = fourpcf_bossS[ang_filt][:,radial_filt]\n",
    "filt_flat_fourpcfN = np.asarray([ff.ravel() for ff in filt_fourpcfN])\n",
    "filt_flat_fourpcfS = np.asarray([ff.ravel() for ff in filt_fourpcfS])\n",
    "filt_flat_fourpcf_bossN = filt_fourpcf_bossN.ravel()\n",
    "filt_flat_fourpcf_bossS = filt_fourpcf_bossS.ravel()\n",
    "\n",
    "# Repeat for disconnected piece\n",
    "filt_fourpcfNdisc = np.asarray([ff[ang_filt][:,radial_filt] for ff in fourpcfNdisc])\n",
    "filt_fourpcfSdisc = np.asarray([ff[ang_filt][:,radial_filt] for ff in fourpcfSdisc])\n",
    "filt_fourpcf_bossNdisc = fourpcf_bossNdisc[ang_filt][:,radial_filt]\n",
    "filt_fourpcf_bossSdisc = fourpcf_bossSdisc[ang_filt][:,radial_filt]\n",
    "filt_flat_fourpcfNdisc = np.asarray([ff.ravel() for ff in filt_fourpcfNdisc])\n",
    "filt_flat_fourpcfSdisc = np.asarray([ff.ravel() for ff in filt_fourpcfSdisc])\n",
    "filt_flat_fourpcf_bossNdisc = filt_fourpcf_bossNdisc.ravel()\n",
    "filt_flat_fourpcf_bossSdisc = filt_fourpcf_bossSdisc.ravel()\n",
    "\n",
    "# Redefine array dimensions\n",
    "n_radial = np.sum(radial_filt)\n",
    "n_mult = np.sum(ang_filt)\n",
    "\n",
    "print(\"N_Lambda: %d\"%n_mult)\n",
    "print(\"N_bins: %d\"%n_radial)\n",
    "print(\"N_total: %d\"%(n_mult*n_radial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Plot Correlation and Covariance Matrices\n",
    "- Theory covariances are computed in the Gaussian Random Field approximation as in Hou et al. (in prep.).\n",
    "- Note that they do not include redshift-space distortions, non-Gaussian contributions, or a proper treatment of the survey geometry.\n",
    "- For this reason they are used only as approximate tools for the compressed-Gaussian analyses below; importantly, we do *not* assume them to be equal to the true covariance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Patchy covariances\n",
      "Loading theory covariances\n",
      "Computing inverse theory covariances\n"
     ]
    }
   ],
   "source": [
    "### Compute Patchy covariances\n",
    "print(\"Computing Patchy covariances\")\n",
    "corrN = np.corrcoef(filt_flat_fourpcfN.T)\n",
    "covN = np.cov(filt_flat_fourpcfN.T)\n",
    "corrS = np.corrcoef(filt_flat_fourpcfS.T)\n",
    "covS = np.cov(filt_flat_fourpcfS.T)\n",
    "\n",
    "### Load theory covariances\n",
    "def load_theory_cov(patch):\n",
    "    cov_file = data_dir+'gaussian_cov_patchy_%s.cov'%patch\n",
    "    cov_dat = pickle.load(open(cov_file, \"rb\"))\n",
    "\n",
    "    # Construct covariance\n",
    "    theory_cov = np.zeros((n_mult*n_radial,n_mult*n_radial))\n",
    "    for i in range(n_mult):\n",
    "        lam1 = '%d%d%d'%(ells[0][ang_filt][i],ells[1][ang_filt][i],ells[2][ang_filt][i])\n",
    "        for j in range(n_mult):\n",
    "            lam2 = '%d%d%d'%(ells[0][ang_filt][j],ells[1][ang_filt][j],ells[2][ang_filt][j])\n",
    "            try: \n",
    "                this_cov = cov_dat[lam1,lam2]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    this_cov = cov_dat[lam2,lam1].T\n",
    "                except KeyError:\n",
    "                    print(\"Covariance element %s,%s has not been computed!\"%(lam1,lam2))\n",
    "            theory_cov[i*n_radial:(i+1)*n_radial,j*n_radial:(j+1)*n_radial] = this_cov[radial_filt][:,radial_filt]\n",
    "    return theory_cov\n",
    "\n",
    "print(\"Loading theory covariances\")\n",
    "theory_covN = load_theory_cov('ngc')\n",
    "theory_covS = load_theory_cov('sgc')\n",
    "\n",
    "# Compute the correlation matrices \n",
    "theory_corrN = theory_covN/np.sqrt(np.outer(np.diag(theory_covN),np.diag(theory_covN)))\n",
    "theory_corrS = theory_covS/np.sqrt(np.outer(np.diag(theory_covS),np.diag(theory_covS)))\n",
    "\n",
    "# Invert the theory covariance\n",
    "print(\"Computing inverse theory covariances\")\n",
    "inv_theory_covN = np.linalg.inv(theory_covN)\n",
    "inv_theory_covS = np.linalg.inv(theory_covS)\n",
    "\n",
    "# Define the eigenvectors from the inverse theory covariance\n",
    "print(\"Performing eigendecomposition\")\n",
    "evalsN,evecsN = np.linalg.eigh(inv_theory_covN)\n",
    "evalsS,evecsS = np.linalg.eigh(inv_theory_covS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Correlation Matrices**\n",
    "- Correlation matrices are defined as $R_{ij} = C_{ij}/\\sqrt{C_{ii}C_{jj}}$\n",
    "- Each submatrix (indicated by the dotted lines) gives a different multiplet, as indicated in green. For example, the second-to-left submatrix on the top line is the correlation of $\\zeta_{000}$ and $\\zeta_{011}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of multiplets to plot\n",
    "n_max = 8\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "# Sample correlation\n",
    "im = ax[0].imshow(corrN[:n_max*n_radial,:n_max*n_radial],vmax=1,vmin=-1,cmap=cm.RdBu_r);\n",
    "# Theory correlation\n",
    "im = ax[1].imshow(theory_corrN[:n_max*n_radial,:n_max*n_radial],vmax=1,vmin=-1,cmap=cm.RdBu_r);\n",
    "for i in range(n_max):\n",
    "    for a in range(2):\n",
    "        if i!=0: ax[a].hlines(n_radial*i-1,0,n_max*n_radial,linestyles='--',alpha=0.4)\n",
    "        if i!=0: ax[a].vlines(n_radial*i-1,0,n_max*n_radial,linestyles='--',alpha=0.4)\n",
    "        if i!=n_max-1: ax[a].text(n_radial*(n_max-1)+n_radial/10+20,n_radial*i+n_radial/2,'%d%d%d'%(ells[0][ang_filt][i],ells[1][ang_filt][i],ells[2][ang_filt][i]),color='darkgreen',size=10)\n",
    "        ax[a].text(n_radial*i+n_radial/10,n_radial*(n_max-1)+n_radial/2+20,'%d%d%d'%(ells[0][ang_filt][i],ells[1][ang_filt][i],ells[2][ang_filt][i]),color='darkgreen',size=10)\n",
    "for a in range(2):\n",
    "    ax[a].set_xlim([0,n_radial*n_max-1])\n",
    "    ax[a].set_ylim([n_radial*n_max-1,0])\n",
    "ax[0].set_title('%d Patchy Mocks'%n_mocks,fontsize=14)\n",
    "ax[1].set_title('Gaussian Theory',fontsize=14);\n",
    "for a in range(2): ax[a].set_xlabel(r'Bin Index 1',fontsize=14)\n",
    "ax[0].set_ylabel(r'Bin Index 2',fontsize=14)\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.03, 0.7])\n",
    "cbar=fig.colorbar(im, cax=cbar_ax)\n",
    "cbar.set_label('Correlation Matrix',fontsize=14)\n",
    "\n",
    "fig.savefig(plot_dir+'correlation_comparison_ngc.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Variances**\n",
    "- We also plot the diagonal elements of the matrices, *i.e.* the 4PCF variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "# Sample variance\n",
    "plt.plot(np.diag(covN)[:n_max*n_radial],c='r',ls='-',label='%d Patchy Mocks'%n_mocks)\n",
    "# Theory variance\n",
    "plt.plot(np.diag(theory_covN)[:n_max*n_radial],c='b',ls='--',label='Gaussian Theory')\n",
    "plt.yscale('log')\n",
    "plt.xlim([0,n_max*n_radial])\n",
    "ylims = [1e-8,1e-4]\n",
    "plt.ylim(ylims)\n",
    "for i in range(n_max):\n",
    "    if i!=0: plt.vlines(n_radial*i,ylims[0],ylims[1],linestyles='--',alpha=0.4)\n",
    "    plt.text(n_radial*i+n_radial/10,ylims[0]*1.3,'%d%d%d'%(ells[0][ang_filt][i],ells[1][ang_filt][i],ells[2][ang_filt][i]),color='darkgreen',size=10) \n",
    "plt.legend(loc='upper left',fontsize=12)\n",
    "plt.xlabel(r'Bin Index',fontsize=13)\n",
    "plt.ylabel(r'Covariance Diagonal',fontsize=13)\n",
    "\n",
    "plt.savefig(plot_dir+'variance_comparison_ngc.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Plot Data\n",
    "- We plot a selection of 4PCF multiplets, $\\zeta_{\\ell_1\\ell_2\\ell_3}(r_1,r_2,r_3)$ below.\n",
    "- The eventual analysis makes use of all even-parity multiplets given the above restrictions.\n",
    "- The three radial bins are collapsed into one-dimension, with the restriction $r_1<r_2<r_3$.\n",
    "- The first panel gives the radial bin centers and the subsequent panels give the multiplets, normalized by $r_1r_2r_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmax = n_radial-1\n",
    "r123 = (radii[0]*radii[1]*radii[2])[radial_filt]\n",
    "\n",
    "# Choose which multiplet indices to plot\n",
    "which_indices = [0,13,21,25]\n",
    "n_plots = len(which_indices)\n",
    "\n",
    "# Plot radial bins in first panel\n",
    "fig,ax = plt.subplots(n_plots+1,figsize=(12,4*n_plots),sharex=True)\n",
    "ax[0].plot(np.arange(n_radial),radii[0][radial_filt],label='Bin 1')\n",
    "ax[0].plot(np.arange(n_radial),radii[1][radial_filt],label='Bin 2')\n",
    "ax[0].plot(np.arange(n_radial),radii[2][radial_filt],label='Bin 3')\n",
    "ax[0].set_xlim([-0.5,xmax+0.5])\n",
    "ax[0].set_ylabel(r'$\\bar r_i$ [$h\\,\\mathrm{Mpc}^{-1}$]',fontsize=13)\n",
    "ax[0].legend(loc='lower right',fontsize=12)\n",
    "\n",
    "# Define mean and variance of mocks\n",
    "fourpcfN_mean = np.mean(filt_fourpcfN,axis=0)\n",
    "fourpcfN_std = np.std(filt_fourpcfN,axis=0)\n",
    "fourpcfS_mean = np.mean(filt_fourpcfS,axis=0)\n",
    "fourpcfS_std = np.std(filt_fourpcfS,axis=0)\n",
    "\n",
    "# Iterate over multiplets to plot\n",
    "for ii,i in enumerate(which_indices):\n",
    "    \n",
    "    # Plot Patchy data\n",
    "    ax[ii+1].fill_between(np.arange(n_radial),r123*(fourpcfN_mean[i]-fourpcfN_std[i]),\n",
    "                     r123*(fourpcfN_mean[i]+fourpcfN_std[i]),color='b',alpha=0.1)\n",
    "    ax[ii+1].plot(np.arange(n_radial),r123*fourpcfN_mean[i],label=r'Patchy: NGC',c='b')\n",
    "\n",
    "    ax[ii+1].fill_between(np.arange(n_radial),r123*(fourpcfS_mean[i]-fourpcfS_std[i]),\n",
    "                     r123*(fourpcfS_mean[i]+fourpcfS_std[i]),color='r',alpha=0.1)\n",
    "    ax[ii+1].plot(np.arange(n_radial),r123*fourpcfS_mean[i],label=r'Patchy: SGC',c='r')\n",
    "        \n",
    "    # Plot BOSS data\n",
    "    ax[ii+1].errorbar(np.arange(n_radial)+0.1,r123*filt_fourpcf_bossN[i],yerr=r123*fourpcfN_std[i],\n",
    "                     ls='',c='b',marker='.',label=r'BOSS: NGC',alpha=0.8)\n",
    "    ax[ii+1].errorbar(np.arange(n_radial)-0.1,r123*filt_fourpcf_bossS[i],yerr=r123*fourpcfS_std[i],\n",
    "                     ls='',c='r',marker='x',label=r'BOSS: SGC',alpha=0.5)\n",
    "\n",
    "    ax[ii+1].hlines(0,0,n_radial,linestyles='--',color='k',alpha=0.4)\n",
    "    \n",
    "    if ii==0: ax[ii+1].legend(loc='lower right',fontsize=12)\n",
    "    ax[ii+1].set_ylabel(r'$r_1r_2r_3\\;\\zeta_{\\ell_1\\ell_2\\ell_3}(r_1,r_2,r_3)$',fontsize=13)\n",
    "    ax[ii+1].set_title(r'$(%d,%d,%d)$'%(ells[0][ang_filt][i],ells[1][ang_filt][i],ells[2][ang_filt][i]),fontsize=14);\n",
    "\n",
    "ax[n_plots].set_xlabel(r'Radial Bin Index',fontsize=13)\n",
    "fig.savefig(plot_dir+'4pcf_main_plot.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Analyze with Rescaled Theory Covariance\n",
    "\n",
    "- One way in which to analyze the data is to use the *theoretical* covariance matrix to form the $\\chi^2$:\n",
    "$$\\chi^2 = \\zeta^T \\mathsf{C}_{\\rm theory}^{-1}\\zeta$$\n",
    "- We can improve the fit of theory and sample covariances by adjusting the effective survey volume and shot-noise.\n",
    "- As in Hou et al. (2021), this is fit using the Kullback-Leibler divergence, comparing the true and theory covariances. Since the fitting procedure is expensive, we provide only the fitted covariances here.\n",
    "- We caution that this is **not** a robust way to analyze the data. As shown in Philcox, Hou & Slepian (2021), it leads to a significant misdetection of non-Gaussianity even when none is present. \n",
    "- The results are included for completeness here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rescaled_theory_cov(patch):\n",
    "    cov_file = data_dir+'fitted_gaussian_cov_patchy_%s.cov'%patch\n",
    "    cov_dat = pickle.load(open(cov_file, \"rb\"))\n",
    "\n",
    "    # Construct covariance\n",
    "    theory_cov = np.zeros((n_mult*n_radial,n_mult*n_radial))\n",
    "    for i in range(n_mult):\n",
    "        lam1 = '%d%d%d'%(ells[0][ang_filt][i],ells[1][ang_filt][i],ells[2][ang_filt][i])\n",
    "        for j in range(n_mult):\n",
    "            lam2 = '%d%d%d'%(ells[0][ang_filt][j],ells[1][ang_filt][j],ells[2][ang_filt][j])\n",
    "            try: \n",
    "                this_cov = cov_dat[lam1,lam2]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    this_cov = cov_dat[lam2,lam1].T\n",
    "                except KeyError:\n",
    "                    print(\"Covariance element %s,%s has not been computed!\"%(lam1,lam2))\n",
    "            theory_cov[i*n_radial:(i+1)*n_radial,j*n_radial:(j+1)*n_radial] = this_cov[radial_filt][:,radial_filt]\n",
    "    return theory_cov\n",
    "\n",
    "### Load fitted theory covariances\n",
    "resc_covN = load_rescaled_theory_cov('ngc')\n",
    "resc_covS = load_rescaled_theory_cov('sgc')\n",
    "\n",
    "print(\"Inverting fitted theory covariances\")\n",
    "inv_resc_covN = np.linalg.inv(resc_covN)\n",
    "inv_resc_covS = np.linalg.inv(resc_covS)\n",
    "\n",
    "# Compute chi^2 using this rescaled model\n",
    "print(\"Computing chi^2\")\n",
    "resc_chi2N = np.sum(filt_flat_fourpcfN.T*np.matmul(inv_resc_covN,filt_flat_fourpcfN.T),axis=0)\n",
    "resc_chi2S = np.sum(filt_flat_fourpcfS.T*np.matmul(inv_resc_covS,filt_flat_fourpcfS.T),axis=0)\n",
    "\n",
    "resc_chi2_bossN = np.sum((filt_flat_fourpcf_bossN)*np.matmul(inv_resc_covN,filt_flat_fourpcf_bossN))\n",
    "resc_chi2_bossS = np.sum((filt_flat_fourpcf_bossS)*np.matmul(inv_resc_covS,filt_flat_fourpcf_bossS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare sample and theory covariances**\n",
    "- We plot the ratio of the Patchy sample covariance to the theory covariance diagonal.\n",
    "- Including the rescaling significantly improves the fit, but it is still imperfect, particularly on small scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.diag(covN)/np.diag(theory_covN),label='Theory')\n",
    "plt.plot(np.diag(covN)/np.diag(resc_covN),label='Rescaled Theory')\n",
    "plt.ylabel(r'$\\mathrm{diag}(C_{\\rm Sample})/\\mathrm{diag}(C_{\\rm Theory})$',fontsize=14)\n",
    "plt.legend(fontsize=13,loc='upper right')\n",
    "plt.xlim([0,n_max*n_radial])\n",
    "plt.ylim([0,3])\n",
    "for i in range(n_max):\n",
    "    if i!=0: plt.vlines(n_radial*i,0,4,linestyles='--',alpha=0.4)\n",
    "    plt.text(n_radial*i+n_radial/10,0.3,'%d%d%d'%(ells[0][ang_filt][i],ells[1][ang_filt][i],ells[2][ang_filt][i]),color='darkgreen',size=10) \n",
    "plt.xlabel(r'Bin Index',fontsize=14)\n",
    "plt.savefig(plot_dir+'fitted_variance_comparison.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the detection PDFs for the NGC and SGC region separately**\n",
    "- This uses the fitted-theory $\\chi^2$s, and is thus not expected to be accurate.\n",
    "- Both the empirical distribution of Patchy mocks and the BOSS data are plotted.\n",
    "- We see the distribution widths are *very* incorrect here, due to the poor choice of covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 25\n",
    "plt.figure()\n",
    "\n",
    "# Plot histograms\n",
    "ct,x,_=plt.hist(resc_chi2N,bins=bins,alpha=0.4,density=1,label=r'%d Patchy Mocks - NGC'%n_mocks,color='r')\n",
    "ct,x,_=plt.hist(resc_chi2S,bins=bins,alpha=0.4,density=1,label=r'%d Patchy Mocks - SGC'%n_mocks,color='b')\n",
    "\n",
    "# Create PDFs of null hypothesis (i.e. chi^2)\n",
    "x_arr = np.arange(min(x)*0.8,max(x)*1.4)\n",
    "p = len(filt_flat_fourpcfN[0])\n",
    "chi2_pdf = chi2.pdf(x_arr,p)\n",
    "\n",
    "# Measure the BOSS detection significance\n",
    "boss_probN = chi2.cdf(resc_chi2_bossN,p)\n",
    "boss_probS = chi2.cdf(resc_chi2_bossS,p)\n",
    "\n",
    "ymax = max([max(ct),max(chi2_pdf)])*1.05\n",
    "print(\"Chi2-BOSS/N_DoF: %.2f (NGC) %.2f (SGC)\"%(resc_chi2_bossN/len(filt_flat_fourpcf_bossN),resc_chi2_bossS/len(filt_flat_fourpcf_bossS)))\n",
    "plt.title(r'BOSS Detection Probability: %.1f%% (NGC) %.1f%% (SGC)'%(100.*boss_probN,100.*boss_probS),fontsize=14)\n",
    "print(\"Effective sigmas: %.2f (NGC) %.2f (SGC)\"%(np.sqrt(-2.*np.log(1.-boss_probN)),np.sqrt(-2.*np.log(1.-boss_probS))))\n",
    "\n",
    "# Plot null hypothesis and BOSS results\n",
    "plt.plot(x_arr,chi2_pdf,color='orange',label=r'$\\chi^2$, $N_\\mathrm{DoF} = %d$'%p)\n",
    "plt.vlines(resc_chi2_bossN,0,ymax,color='r',linestyles='--',label='BOSS Data - NGC')\n",
    "plt.vlines(resc_chi2_bossS,0,ymax,color='b',linestyles='--',label='BOSS Data - SGC')\n",
    "plt.legend(fontsize=12,bbox_to_anchor=(1.,1.));\n",
    "plt.ylim([0,ymax])\n",
    "plt.xlabel(r'Theoretical Covariance $\\chi^2$',fontsize=14)\n",
    "plt.ylabel(r'PDF',fontsize=14)\n",
    "plt.savefig(plot_dir+'fitted_chi2_ngc_sgc.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Perform data compression\n",
    "\n",
    "- We now analyze the data using the compression scheme in Philcox et al. (2021), based on Scoccimarro (2000).\n",
    "- Data are projected onto the eigenvectors of the *model* inverse covariance matrix.\n",
    "- This would be an optimal projection if true = model covariance, in the Gaussian limit, and is always unbiased.\n",
    "- Here, $v = U^T\\zeta$ where $\\tilde{\\mathsf C} = U\\Lambda U^T$, and we keep the first $N_\\mathrm{eig}$ eigenvectors, which are ordered in signal-to-noise.\n",
    "- The covariance matrix of $v$ is *close* to diagonal, with $\\langle vv^T \\rangle = U^TC_DU$. If $C_D = \\tilde{\\mathsf{C}} = U\\Lambda U^T$, then $\\langle vv^T \\rangle = \\Lambda$, which is diagonal.\n",
    "\n",
    "- This is performed for NGC and SGC independently, since they are assumed to be independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define signal mean (used for S/N vectors)\n",
    "fourpcf_meanN = np.mean(filt_flat_fourpcfN,axis=0)\n",
    "fourpcf_meanS = np.mean(filt_flat_fourpcfS,axis=0)\n",
    "\n",
    "# Compute (squared) signal-to-noise of each eigenvector\n",
    "SN_N = np.matmul(evecsN.T,fourpcf_meanN)**2.*evalsN\n",
    "SN_S = np.matmul(evecsS.T,fourpcf_meanS)**2.*evalsS\n",
    "\n",
    "# Order the eigenvectors in termms of signal-to-noise\n",
    "orderN = np.argsort(SN_N)[::-1]\n",
    "orderS = np.argsort(SN_S)[::-1]\n",
    "evalsN = evalsN[orderN]\n",
    "evalsS = evalsS[orderS]\n",
    "evecsN = evecsN[:,orderN]\n",
    "evecsS = evecsS[:,orderS]\n",
    "\n",
    "def project_data(data,N_eig,patch='ngc'):\n",
    "    \"\"\"Project data onto the eigenvector basis for NGC or SGC.\"\"\"\n",
    "    if patch=='ngc':\n",
    "        these_evecs = evecsN[:,:N_eig]\n",
    "    else:\n",
    "        these_evecs = evecsS[:,:N_eig]\n",
    "    proj_data = np.matmul(these_evecs.T,data.T).T\n",
    "    return proj_data\n",
    "\n",
    "# Project onto the basis vectors for Patchy and BOSS\n",
    "N_eig = 50\n",
    "projected_fourpcfN = project_data(filt_flat_fourpcfN,N_eig,'ngc')\n",
    "projected_fourpcfS = project_data(filt_flat_fourpcfS,N_eig,'sgc')\n",
    "projected_fourpcf_bossN = project_data(filt_flat_fourpcf_bossN,N_eig,'ngc')\n",
    "projected_fourpcf_bossS = project_data(filt_flat_fourpcf_bossS,N_eig,'sgc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compressed Covariance**\n",
    "- We plot the covariance matrix of the compressed 4PCF. \n",
    "- This should be close to diagonal if the projection is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(1e5*np.cov(projected_fourpcfN.T),cmap=cm.RdBu_r,vmax=2,vmin=-2);\n",
    "fs = 14\n",
    "cbar = plt.colorbar();\n",
    "plt.xlabel(r'Eigenvalue Index',fontsize=fs)\n",
    "plt.ylabel(r'Eigenvalue Index',fontsize=fs)\n",
    "cbar.set_label(r'$10^5\\times$ Projected Covariance',fontsize=fs)\n",
    "plt.savefig(plot_dir+'projected_4pcf_covN.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compressed Data**\n",
    "- We visualize the compressed data directly. \n",
    "- A robust detection of non-Gaussianity is seen from data-points that are well separated from zero (either above or below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanN = projected_fourpcfN.mean(axis=0)\n",
    "stdN = projected_fourpcfN.std(axis=0)\n",
    "meanS = projected_fourpcfS.mean(axis=0)\n",
    "stdS = projected_fourpcfS.std(axis=0)\n",
    "i_arr = np.arange(len(meanN))\n",
    "plt.errorbar(i_arr,meanN,label='Patchy: NGC',c='b')\n",
    "plt.errorbar(i_arr,meanS,label='Patchy: SGC',c='r')\n",
    "plt.errorbar(i_arr,projected_fourpcf_bossN,ls='',marker='x',alpha=0.8,\n",
    "             yerr=stdN,label='BOSS Data',c='b')\n",
    "plt.errorbar(i_arr,projected_fourpcf_bossS,ls='',marker='x',alpha=0.5,\n",
    "             yerr=stdS,label='BOSS Data',c='r')\n",
    "plt.fill_between(i_arr,meanN-stdN,meanN+stdN,alpha=0.1,color='b')\n",
    "plt.fill_between(i_arr,meanS-stdS,meanS+stdS,alpha=0.1,color='r')\n",
    "plt.xlabel(r'Eigenvalue Index',fontsize=14)\n",
    "plt.ylabel(r'Projected 4PCF',fontsize=14)\n",
    "plt.xlim([-0.5,N_eig-0.5])\n",
    "plt.legend(fontsize=12)\n",
    "vmax = np.max([np.max(projected_fourpcf_bossS+stdS),np.max(projected_fourpcf_bossN+stdN)])\n",
    "plt.ylim([-vmax*2,vmax*2])\n",
    "plt.savefig(plot_dir+'projected_4pcf_data.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Perform a Gaussian hypothesis test for the projected statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given projected data $v$, we form the test statistic:\n",
    "\n",
    "$$T^2 = v^T\\hat{\\mathsf{C}}_v^{-1}v,$$\n",
    "where $\\hat{\\mathsf{C}}$ is the projected sample covariance matrix, and we assume zero sample mean, matching the null hypothesis of no non-Gaussianity. \n",
    "- As in Sellentin & Heavens (2016), this follows a modified $F$-distribution, such that\n",
    "$$T^2\\sim \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\Gamma(p/2)\\Gamma[(n-p+1)/2]}\\frac{n^{-p/2}(T^2)^{p/2-1}}{(T^2/n+1)^{(n+1)/2}}$$\n",
    "where $p = \\mathrm{dim}(v)$ and $n = N_\\mathrm{mocks}-1$.\n",
    "- We can also form the conventional $\\chi^2$ statistic, including the Hartlap factor required to debias noisy inverse covariance matrices:\n",
    "$$H^2 = f_H\\times v^T\\hat{\\mathsf{C}}^{-1}v,$$ where $f_H = (n-p-1)/n$. This is usually assumed to follow $\\chi^2$ statistics, *i.e.* $H^2\\sim \\chi^2_p$, but this breaks down at small $n$.\n",
    "- Both distributions are considered below. To form empirical distributions we apply jackknifing, computing the covariance from $(N-1)$ mocks and repeating.\n",
    "- Below, analysis is performed for several choices of $N_\\mathrm{eig}$, and we plot the detection PDFs and CDFs.\n",
    "- The independent NGC and SGC measurements are summed, with their theoretical distributions obtained via a convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numbers of basis vectors to test\n",
    "N_eigs = [10,50]\n",
    "\n",
    "# Number of histogram bins\n",
    "bins = 25\n",
    "\n",
    "fig1,ax1 = plt.subplots(1,len(N_eigs),figsize=(5*len(N_eigs),4))\n",
    "fig2,ax2 = plt.subplots(1,len(N_eigs),figsize=(5*len(N_eigs),4),sharey=True)\n",
    "\n",
    "for nn,N_eig in enumerate(N_eigs):\n",
    "    print(\"\\n\\nN_eig = %d\"%N_eig)\n",
    "    \n",
    "    # Project the data onto the basis\n",
    "    projected_fourpcfN = project_data(filt_flat_fourpcfN,N_eig,'ngc')\n",
    "    projected_fourpcfS = project_data(filt_flat_fourpcfS,N_eig,'sgc')\n",
    "    projected_fourpcf_bossN = project_data(filt_flat_fourpcf_bossN,N_eig,'ngc')\n",
    "    projected_fourpcf_bossS = project_data(filt_flat_fourpcf_bossS,N_eig,'sgc')\n",
    "    \n",
    "    ### Compute distribution of mock data using sets of (N-1) mocks, i.e. jackknifing\n",
    "    this_n_mocks = len(projected_fourpcfN)-1\n",
    "    \n",
    "    all_T2, all_H2 = [],[]\n",
    "    print(\"Jackknifing to compute empirical Patchy covariances. This may take a while...\")\n",
    "    for i in range(len(projected_fourpcfN)):\n",
    "        if (i+1)%25==0: print(\"On step %d of %d\"%(i+1,len(projected_fourpcfN)))\n",
    "\n",
    "        # Define data\n",
    "        projected_mockN = projected_fourpcfN[i]\n",
    "        projected_mockS = projected_fourpcfS[i]\n",
    "\n",
    "        # Define sample covariance in projected space\n",
    "        projected_covN = np.cov(projected_fourpcfN[np.arange(len(projected_fourpcfN))!=i].T)\n",
    "        projected_covS = np.cov(projected_fourpcfS[np.arange(len(projected_fourpcfS))!=i].T)\n",
    "        \n",
    "        # Invert the covariance\n",
    "        inv_projected_covN = np.linalg.inv(projected_covN)\n",
    "        inv_projected_covS = np.linalg.inv(projected_covS)\n",
    "\n",
    "        # Compute the sample statistic\n",
    "        T2_statistic = np.inner(projected_mockN,np.inner(inv_projected_covN,projected_mockN))\n",
    "        T2_statistic += np.inner(projected_mockS,np.inner(inv_projected_covS,projected_mockS))\n",
    "        \n",
    "        all_T2.append(T2_statistic)\n",
    "        \n",
    "    all_T2 = np.asarray(all_T2)\n",
    "    \n",
    "    # Compute also Hartlap-rescaled statistic\n",
    "    hartlap_factor = (this_n_mocks-N_eig-2.)/(this_n_mocks-1)\n",
    "    print(\"Hartlap Factor for jackknifed PDF: %.3f\"%hartlap_factor)\n",
    "    all_H2 = all_T2*hartlap_factor\n",
    "    \n",
    "    ### Compute T2 and H2 statistics on BOSS data using every mmock\n",
    "    projected_covN = np.cov(projected_fourpcfN.T)\n",
    "    projected_covS = np.cov(projected_fourpcfS.T)\n",
    "    inv_projected_covN = np.linalg.inv(projected_covN)\n",
    "    inv_projected_covS = np.linalg.inv(projected_covS)\n",
    "    T2_boss = np.inner(projected_fourpcf_bossN,np.inner(inv_projected_covN,projected_fourpcf_bossN))\n",
    "    T2_boss += np.inner(projected_fourpcf_bossS,np.inner(inv_projected_covS,projected_fourpcf_bossS))\n",
    "    hartlap_factor = (n_mocks-N_eig-2.)/(n_mocks-1)\n",
    "    print(\"Hartlap Factor for BOSS: %.3f\"%hartlap_factor)\n",
    "    H2_boss = T2_boss*hartlap_factor\n",
    "    \n",
    "    ### Compute PDFs and CDFs (easiest to do CDFs numerically with a fine grid)\n",
    "    mmax = max(all_T2)*1.5\n",
    "    x_arr = np.linspace(0.001,mmax,10000)\n",
    "    p = N_eig\n",
    "    n = n_mocks-1\n",
    "    chi2_single_pdf = chi2.pdf(x_arr,p)\n",
    "    T2_single_pdf = np.float128(np.exp((p/2.-1.)*np.log(x_arr)-p/2.*np.log(n)-0.5*(n+1.)*np.log(x_arr/n+1.))*np.exp(loggamma((n+1.)/2.)-loggamma(p/2.)-loggamma((n-p+1.)/2.)))\n",
    "\n",
    "    # Compute PDF of H^2_NGC + H^2_SGC using a convolution\n",
    "    chi2_pdf = np.fft.ifft(np.fft.fft(chi2_single_pdf)**2.).real\n",
    "    chi2_pdf /= np.sum(chi2_pdf)*np.diff(x_arr)[0]\n",
    "\n",
    "    # Compute PDF of T^2_NGC + T^2_SGC using a convolution\n",
    "    T2_pdf = np.fft.ifft(np.fft.fft(T2_single_pdf)**2.).real\n",
    "    T2_pdf /= np.sum(T2_pdf)*np.diff(x_arr)[0]\n",
    "\n",
    "    # Compute CDFs\n",
    "    chi2_cdf = interp1d(x_arr,np.cumsum(chi2_pdf)*np.diff(x_arr)[0])\n",
    "    T2_cdf = interp1d(x_arr,np.cumsum(T2_pdf)*np.diff(x_arr)[0])\n",
    "\n",
    "    ### Plot empirical distributions\n",
    "    ct,_,_=ax1[nn].hist(all_H2,bins=bins,alpha=0.4,density=1,color='orange')#,label=r'$H^2$')\n",
    "    ax1[nn].hist(all_T2,bins=bins,alpha=0.4,density=1,color='blue')#,label=r'$T^2$');\n",
    "    ax1[nn].vlines(T2_boss,0,2*mmax,color='blue',linestyles='--')\n",
    "    ax1[nn].vlines(H2_boss,0,2*mmax,color='orange',linestyles='--')\n",
    "    \n",
    "    ### Plot theoretical distributions\n",
    "    ax1[nn].plot(x_arr,chi2_pdf,color='orange',label=r'$H^2$')\n",
    "    ax1[nn].plot(x_arr,T2_pdf,color='blue',label=r'$T^2$')\n",
    "    \n",
    "    ### Add cosmetics\n",
    "    if nn==0:\n",
    "        ax1[nn].legend(fontsize=12);\n",
    "    ax1[nn].set_xlim([0,mmax])\n",
    "    ax1[nn].set_ylim([0,max([max(ct),max(chi2_pdf)])*1.05])\n",
    "    ax1[nn].set_xlabel(r'$H^2$ or $T^2$',fontsize=14)\n",
    "    if nn==0: ax1[nn].set_ylabel(r'PDF',fontsize=14)\n",
    "    ax1[nn].set_title(r'$N_\\mathrm{eig} = %d$'%N_eig,fontsize=14);\n",
    "    if nn==2: fig1.savefig(plot_dir+'projected_pdfs_all.pdf',bbox_inches='tight')\n",
    "\n",
    "    ### Compute detection probabilities, as a 1-tail test\n",
    "    prob_T2 = T2_cdf(all_T2)\n",
    "    prob_chi2 = chi2_cdf(all_H2)\n",
    "\n",
    "    prob_T2_boss = T2_cdf(T2_boss)\n",
    "    prob_chi2_boss = chi2_cdf(H2_boss)\n",
    "\n",
    "    # Count fraction of false detections \n",
    "    N_detections_T2 = (np.sum(prob_T2>0.95))*1./n_mocks\n",
    "    N_detections_chi2 = (np.sum(prob_chi2>0.95))*1./n_mocks\n",
    "\n",
    "    print(\"\\nFraction of 2-sigma detections with %d eigenvalues\"%N_eig)\n",
    "    print(\"chi^2 + Hartlap model: %.3f\"%N_detections_chi2)\n",
    "    print(\"T^2: %.3f\"%N_detections_T2)\n",
    "    print(\"Expected: %.3f\"%(0.05))\n",
    "\n",
    "    print(\"\\nBOSS Detection Rates:\")\n",
    "    print(\"chi^2 + Hartlap model: %.3f\"%prob_chi2_boss)\n",
    "    print(\"T^2: %.3f\"%prob_T2_boss)\n",
    "    print(\"Effective Sigmas: %.2f\"%np.sqrt(-2.*np.log(1.-prob_T2_boss)))\n",
    "\n",
    "    ### Histogram the detection PDFs\n",
    "    ax2[nn].set_title(r'BOSS: %.1f%%'%(100.*prob_T2_boss),fontsize=14)\n",
    "    ct,_,_=ax2[nn].hist(prob_chi2,bins=bins,histtype='step',color='orange',label=r'$H^2$',density=0,range=[0,1.00001]);\n",
    "    ax2[nn].hist(prob_T2,bins=bins,histtype='step',color='blue',label=r'$T^2$',density=0,range=[0,1.00001]);\n",
    "    ax2[nn].hlines(n_mocks/bins,0,1,linestyles='--')\n",
    "    ax2[nn].vlines(prob_chi2_boss,0,max(ct)*1.2,color='orange',linestyles='--')\n",
    "    ax2[nn].vlines(prob_T2_boss,0,max(ct)*1.2,color='blue',linestyles='--')\n",
    "    ax2[nn].set_xlim([0.0,1])\n",
    "    ax2[nn].fill_betweenx(np.arange(0,max(ct)*2),0.95,1,alpha=0.1,color='k')\n",
    "    ax2[nn].set_ylim([0,max(ct)*1.1])\n",
    "    if nn==0: ax2[nn].legend(fontsize=12,loc='lower right');\n",
    "    if nn==0: ax2[nn].set_ylabel(r'Occurences',fontsize=14)\n",
    "    ax2[nn].set_xlabel(r'Detection Probability',fontsize=14);\n",
    "    if nn==2:\n",
    "        fig2.savefig(plot_dir+'projected_cdfs_all.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This completes the analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptenv [~/.conda/envs/ptenv/]",
   "language": "python",
   "name": "conda_ptenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
